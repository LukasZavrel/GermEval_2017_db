{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) is an area of computer science and artificial intelligence that is concerned with the interaction between computers and humans in natural language. The ultimate goal of NLP is to enable computers to understand language as well as we do. It is the driving force behind things like virtual assistants, speech recognition, sentiment analysis, automatic text summarization, machine translation and much more.\n",
    "(https://towardsdatascience.com/introduction-to-nlp-5bff2b2a7170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLP number of publications](NLP_publications.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todays Task: GermEval - Identification of Offensive Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to decide whether a tweet includes:\n",
    "\n",
    "1. no form of offensive language- marked OTHER\n",
    "\n",
    "2. some form of offensive language - marked OFFENSE\n",
    "    * PROFANITY: usage of profane words, however, the tweet clearly does not want to insult anyone.\n",
    "    \n",
    "    * INSULT: unlike PROFANITY the tweet clearly wants to offend someone.\n",
    "    \n",
    "    * ABUSE: unlike INSULT, the tweet does not just insult but represents the stronger form of abusive language.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Couple of examples"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@ahrens_theo fr√∂hlicher gru√ü aus der sch√∂nsten stadt der welt theo ‚öìÔ∏è\tOTHER\tOTHER\n",
    "@SteinbachErika Das lassen die üá©üá™ nicht zu....und wenn sie gegen ihren eigenen Willen festgehalten werden... |LBR| Es ist leicht nach üá©üá™einzureisen...aber raus?\tOTHER\tOTHER\n",
    "\n",
    "@TiffanyAngelx zu anbeissen ,dein geiler Arsch\tOFFENSE\tPROFANITY\n",
    "\n",
    "@anna_IIna Kann man diesen ganzen Schei√ü noch glauben..?\tOFFENSE\tPROFANITY\n",
    "\n",
    "@Ralf_Stegner Oman Ralle..dich mag ja immer noch keiner. Du willst das die Hetze gegen dich aufh√∂rt? |LBR| Geh in Rente und verzichte auf die 1/2deiner Pension\tOFFENSE\tINSULT\n",
    "\n",
    "Infotweet: Antifaschisten sind alle vernachl√§ssigte Scheidungskinder\tOFFENSE\tINSULT\n",
    "\n",
    "Deutsche Medien, Halbwahrheiten und einseitige Betrachtung, wie bei allen vom Staat finanzierten \"billigen\" Propagandainstitutionen üòú\tOFFENSE\tABUSE\n",
    "\n",
    "@krippmarie Ich kenne noch einige Namen unter den SPDler die ebenfalls zu Grabe getragen m√ºssten-sollten-werden....\tOFFENSE\tABUSE\n",
    "\n",
    "Ich pers√∂nlich scheisse auf die gr√ºne Kinderfickerpartei\tOFFENSE\tABUSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T12:46:38.082921Z",
     "start_time": "2019-05-21T12:46:38.010715Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def get_train_data(filename):\n",
    "    X  = []\n",
    "    y_task1 = []\n",
    "    y_task2 = []\n",
    "    \n",
    "    with open(filename, encoding='UTF-8') as file:\n",
    "        for line in file:\n",
    "            #rstrip - remove trailing characters (\\n is a new line)\n",
    "            #split into more parts (separator is tab: \\t)\n",
    "            tweet = line.rstrip('\\n').split('\\t')\n",
    "            X.append(tweet[0]) # the actual tweet\n",
    "            y_task1.append(tweet[1]) # first label\n",
    "            y_task2.append(tweet[2]) # second label\n",
    "    \n",
    "    return np.asarray(X), np.asarray(y_task1), np.asarray(y_task2)\n",
    "\n",
    "filename = \"./data/germeval2018.training.txt\"\n",
    "X_train, Y_train1, Y_train2 = get_train_data(filename)\n",
    "filename = \"./data/germeval2018.test.txt\"\n",
    "X_test, Y_test1, Y_test2 = get_train_data(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the first ten tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T12:46:39.292103Z",
     "start_time": "2019-05-21T12:46:39.278804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@corinnamilborn Liebe Corinna, wir w√ºrden dich gerne als Moderatorin f√ºr uns gewinnen! W√§rst du begeisterbar?',\n",
       "       '@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverst√§ndlich. Dass das BVerfG Sachleistungen nicht ausschlie√üt, kritisieren wir.',\n",
       "       '@ahrens_theo fr√∂hlicher gru√ü aus der sch√∂nsten stadt der welt theo ‚öìÔ∏è',\n",
       "       '@dushanwegner Amis h√§tten alles und jeden gew√§hlt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!',\n",
       "       '@spdde kein verl√§√ülicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgespr√§chen - schickt diese St√ºmper #SPD in die Versenkung.',\n",
       "       '@Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Gesch√ºtzte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bem√ºht - √ºbrigens leicht r√ºckl√§ufig gewesen.',\n",
       "       '@milenahanm 33 bis 45 habe ich noch gar nicht gelebt und es geht mir am Arsch vorbei was in dieser Zeit geschehen ist. Ich lebe im heute und jetzt und nicht in der Vergangenheit.',\n",
       "       '@jayxderxmensch @jayxthexhuman Wieso? Was findest du da unklar?',\n",
       "       '@tagesschau Euere AfD Hetze wirkt. Da k√∂nnt ihr stolz sein bei #ARD-Fernsehen',\n",
       "       'Deutsche Medien, Halbwahrheiten und einseitige Betrachtung, wie bei allen vom Staat finanzierten \"billigen\" Propagandainstitutionen üòú'],\n",
       "      dtype='<U637')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T12:46:40.309505Z",
     "start_time": "2019-05-21T12:46:40.283204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OTHER      3321\n",
       "OFFENSE    1688\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "OTHER        3321\n",
       "ABUSE        1022\n",
       "INSULT        595\n",
       "PROFANITY      71\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.Series(Y_train1).value_counts())\n",
    "display(pd.Series(Y_train2).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to start tackling this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using only data given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply look at each word and count how many times it is in an offensive tweet in comparison to its presence in an non offensive tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T12:46:45.691806Z",
     "start_time": "2019-05-21T12:46:43.452447Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy.sparse import find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:31:46.116857Z",
     "start_time": "2019-05-21T13:31:46.110599Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def create_dist(text):\n",
    "    c = Counter(text)\n",
    "\n",
    "    total = sum(c.values())\n",
    "    \n",
    "    for k, v in c.items():\n",
    "        c[k] = v/total\n",
    "\n",
    "    return defaultdict(lambda: min(c.values()), c)\n",
    "\n",
    "list_of_others = []\n",
    "list_of_offensive = []\n",
    "for number, polarity in enumerate(Y_train1):\n",
    "    if polarity == \"OTHER\":\n",
    "        list_of_others.append(number)\n",
    "    else:\n",
    "        list_of_offensive.append(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T12:46:45.852790Z",
     "start_time": "2019-05-21T12:46:45.694622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The shape of the data is: (5009, 17534)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Sparse vector representation of the tweet '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'@dushanwegner Amis h√§tten alles und jeden gew√§hlt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([  664,   747,  3872,  4033,  4644,  5286,  6361,  7213,  7504,\n",
       "         8019, 10865, 11034, 11076, 11669, 12199, 13480, 15077, 16084,\n",
       "        16817], dtype=int32),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transform the tweets into a sparse matrix\n",
    "count_vect = CountVectorizer(min_df=1)\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "\n",
    "display(\"The shape of the data is: \" + str(X_train_counts.shape))\n",
    "\n",
    "display(\"Sparse vector representation of the tweet \", X_train[3])\n",
    "\n",
    "display(find(X_train_counts[3])[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:46:05.777369Z",
     "start_time": "2019-05-21T13:46:05.537567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alles', 1, 0.001696005614363413, 0.0017457619332016355),\n",
       " ('amis', 1, 5.8482952219428036e-05, 3.06274023368708e-05),\n",
       " ('dushanwegner', 1, 0.00023393180887771215, 0.00016845071285278939),\n",
       " ('eine', 1, 0.004854085034212527, 0.004456287040014701),\n",
       " ('erst', 1, 0.0010819346160594186, 0.0006431754490742867),\n",
       " ('fortsetzung', 1, 0.0, 1.53137011684354e-05),\n",
       " ('gew√§hlt', 1, 0.00032165623720685423, 0.00038284252921088496),\n",
       " ('hillary', 1, 0.0, 0.0001071959081790478),\n",
       " ('h√§tten', 1, 0.0001754488566582841, 0.00039815623037932037),\n",
       " ('jeden', 1, 0.0002924147610971402, 0.00016845071285278939),\n",
       " ('nicht', 2, 0.011374934206678754, 0.012587862360453898),\n",
       " ('nur', 1, 0.005906778174162232, 0.0040581308096353805),\n",
       " ('obama', 1, 0.0, 6.12548046737416e-05),\n",
       " ('politik', 1, 0.0007895198549622785, 0.0007044302537480284),\n",
       " ('recht', 1, 0.0005555880460845664, 0.0009035083689376886),\n",
       " ('sie', 1, 0.006725539505234224, 0.009096338494050627),\n",
       " ('und', 2, 0.020732206561787238, 0.01764138374603758),\n",
       " ('von', 1, 0.007076437218550793, 0.0063398722837322554),\n",
       " ('wollten', 1, 0.0001462073805485701, 0.0001071959081790478)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweet_words = find(X_train_counts[3])[1]\n",
    "words = [count_vect.get_feature_names()[pos] for pos in tweet_words]\n",
    "offensive_prob = [offensive_counts[pos] for pos in tweet_words]\n",
    "non_offensive_prob = [non_offensive_counts[pos] for pos in tweet_words]\n",
    "display([(x, y, off, other) for x,y, off, other in zip(words, find(X_train_counts[3])[2], offensive_prob, non_offensive_prob)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:37:02.396823Z",
     "start_time": "2019-05-21T13:37:02.389882Z"
    }
   },
   "outputs": [],
   "source": [
    "non_offensive_counts = np.array(X_train_counts[list_of_others].sum(axis = 0)).flatten()\n",
    "non_offensive_counts = non_offensive_counts / non_offensive_counts.sum()\n",
    "offensive_counts = np.array(X_train_counts[list_of_offensive].sum(axis = 0)).flatten()\n",
    "offensive_counts = offensive_counts / offensive_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:37:07.150739Z",
     "start_time": "2019-05-21T13:37:07.145548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.84829522e-05, 1.75448857e-04, 0.00000000e+00, ...,\n",
       "       2.92414761e-05, 0.00000000e+00, 2.92414761e-05])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offensive_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:37:14.015458Z",
     "start_time": "2019-05-21T13:37:14.010895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.12548047e-05, 2.29705518e-04, 3.06274023e-05, ...,\n",
       "       0.00000000e+00, 1.53137012e-05, 0.00000000e+00])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_offensive_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Couple of terms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF: Document Frequency - df(t): how many documents contain term t\n",
    "\n",
    "TF: Term Frequency - tf(t,d): the number of times that term t occurs in document d\n",
    "\n",
    "IDF: Inverse Document Frequency - idf(t, D) = log (|D|/df(t))+1: measure of how much information the term t provides in the set of all documents D\n",
    "\n",
    "Goal: Scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n",
    "\n",
    "tf-idf(t, d, D) = tf(t, d) * idf(t, D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T13:43:13.253911Z",
     "start_time": "2019-05-15T13:43:13.245209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alles', 0.17160673491699563),\n",
       " ('amis', 0.31007615417674533),\n",
       " ('dushanwegner', 0.25573099210317224),\n",
       " ('eine', 0.13582432850827794),\n",
       " ('erst', 0.20288416837938814),\n",
       " ('fortsetzung', 0.34599635245987703),\n",
       " ('gew√§hlt', 0.23161463976175056),\n",
       " ('hillary', 0.29165119038630394),\n",
       " ('h√§tten', 0.23730602831273087),\n",
       " ('jeden', 0.2519946664612334),\n",
       " ('nicht', 0.20025121923204178),\n",
       " ('nur', 0.13429034749981186),\n",
       " ('obama', 0.31007615417674533),\n",
       " ('politik', 0.20606443153617798),\n",
       " ('recht', 0.20444205872496402),\n",
       " ('sie', 0.11795542720298555),\n",
       " ('und', 0.17114742491461543),\n",
       " ('von', 0.12285815231128688),\n",
       " ('wollten', 0.2726184353837312)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "display([(x,y) for x, y in zip(words, find(X_train_tfidf[3])[2])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing polarity of each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now able to characterize the polarity of each word by its inclusion in the offensive tweets vs non offensive ones. An example: the word \"peinlich\" is contained in 2 offensive and 5 non offensive tweets, we can thus compute its offensiveness as 2/(2+5) = 2/7. We will furthermore multiply this by its tf-idf statistics to lower the importance of the common words for the classification. The product of these results for all words in the tweet gives us a number characterizing the probability of an offensive tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T14:18:17.334894Z",
     "start_time": "2019-05-15T14:18:16.271295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 0.01\n",
      "tfidf__use_idf: False\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6943130807402307"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.30255985]),\n",
       " 'std_fit_time': array([0.08236278]),\n",
       " 'mean_score_time': array([0.02378073]),\n",
       " 'std_score_time': array([0.00740208]),\n",
       " 'param_clf__alpha': masked_array(data=[0.01],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tfidf__use_idf': masked_array(data=[False],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__ngram_range': masked_array(data=[(1, 1)],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'clf__alpha': 0.01,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__ngram_range': (1, 1)}],\n",
       " 'split0_test_score': array([0.67820978]),\n",
       " 'split1_test_score': array([0.66089269]),\n",
       " 'split2_test_score': array([0.73644201]),\n",
       " 'split3_test_score': array([0.70977208]),\n",
       " 'split4_test_score': array([0.6771315]),\n",
       " 'split5_test_score': array([0.71850876]),\n",
       " 'split6_test_score': array([0.66695157]),\n",
       " 'split7_test_score': array([0.67001162]),\n",
       " 'split8_test_score': array([0.72648107]),\n",
       " 'split9_test_score': array([0.69872973]),\n",
       " 'mean_test_score': array([0.69431308]),\n",
       " 'std_test_score': array([0.02581512]),\n",
       " 'rank_test_score': array([1], dtype=int32),\n",
       " 'split0_train_score': array([0.98480474]),\n",
       " 'split1_train_score': array([0.98507269]),\n",
       " 'split2_train_score': array([0.9863054]),\n",
       " 'split3_train_score': array([0.98427184]),\n",
       " 'split4_train_score': array([0.98578395]),\n",
       " 'split5_train_score': array([0.98581673]),\n",
       " 'split6_train_score': array([0.9872888]),\n",
       " 'split7_train_score': array([0.98579336]),\n",
       " 'split8_train_score': array([0.98579494]),\n",
       " 'split9_train_score': array([0.9853159]),\n",
       " 'mean_train_score': array([0.98562484]),\n",
       " 'std_train_score': array([0.00079088])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# Use Multinomial naive Bayes\n",
    "#clf = MultinomialNB().fit(X_train_tfidf, Y_train1)\n",
    "\n",
    "# Characterize the order of operations\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Set hyperparameters\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1)],\n",
    "    'tfidf__use_idf': [(False)],\n",
    "    'clf__alpha': [(1e-2)], #smoothing\n",
    "}\n",
    "\n",
    "# Find the best parameters\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=10, iid=False,\n",
    "                      n_jobs=-1, scoring=make_scorer(f1_score, average='macro'))\n",
    "gs_clf = gs_clf.fit(X_train, Y_train1)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "\n",
    "\n",
    "display(gs_clf.best_score_)\n",
    "display(gs_clf.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting better, part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngrams: Using a word count does not always works, an example of two sentences with same words:\n",
    "\n",
    "\"Alle nicht t√∂ten und leben lassen vs Alle t√∂ten und nicht leben lassen\"\n",
    "\n",
    "A solution: use group of words (2-5) so called n-grams to better represent the context. For the choice n=2 we have tokens: Alle nicht, nicht t√∂ten, t√∂ten und, und leben, leben lassen, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T11:55:51.138656Z",
     "start_time": "2019-05-13T11:55:31.407303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 0.01\n",
      "tfidf__use_idf: False\n",
      "vect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=10, iid=False,\n",
    "                      n_jobs=-1, scoring=make_scorer(f1_score, average='macro'))\n",
    "gs_clf.fit(X_train, Y_train1)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T11:56:22.975455Z",
     "start_time": "2019-05-13T11:56:22.953987Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/lukas/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.25981333, 0.69109664, 1.1778029 , 0.24055579, 0.64192955,\n",
       "        1.17820904, 0.2945739 , 0.77496316, 1.33338332, 0.30388579,\n",
       "        1.10317478, 1.43830168]),\n",
       " 'std_fit_time': array([0.00734382, 0.02066649, 0.06060069, 0.00587286, 0.0134585 ,\n",
       "        0.0599287 , 0.01731191, 0.00960534, 0.04687888, 0.03156629,\n",
       "        0.11497705, 0.2490255 ]),\n",
       " 'mean_score_time': array([0.02124653, 0.0414655 , 0.04980199, 0.02066953, 0.03777406,\n",
       "        0.0539731 , 0.02496397, 0.04513886, 0.05968206, 0.02507839,\n",
       "        0.05951574, 0.06048067]),\n",
       " 'std_score_time': array([0.00119077, 0.00390739, 0.00170173, 0.00273915, 0.00148889,\n",
       "        0.00426007, 0.00085359, 0.00219275, 0.00550332, 0.00310754,\n",
       "        0.0204003 , 0.01968232]),\n",
       " 'param_clf__alpha': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tfidf__use_idf': masked_array(data=[True, True, True, False, False, False, True, True,\n",
       "                    True, False, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 3), (1, 1), (1, 2), (1, 3), (1, 1),\n",
       "                    (1, 2), (1, 3), (1, 1), (1, 2), (1, 3)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'clf__alpha': 0.01,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 3)},\n",
       "  {'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 3)},\n",
       "  {'clf__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 3)},\n",
       "  {'clf__alpha': 0.001, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 0.001, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 0.001, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 3)}],\n",
       " 'split0_test_score': array([0.682122  , 0.71020349, 0.68676127, 0.67820978, 0.7247807 ,\n",
       "        0.71066282, 0.67390258, 0.70199615, 0.68579084, 0.67468786,\n",
       "        0.71186386, 0.70282873]),\n",
       " 'split1_test_score': array([0.66662324, 0.6764402 , 0.67128585, 0.66089269, 0.68592575,\n",
       "        0.66915818, 0.65607532, 0.66836659, 0.65483092, 0.65571253,\n",
       "        0.66472817, 0.66821192]),\n",
       " 'split2_test_score': array([0.72539019, 0.74831019, 0.74164978, 0.73644201, 0.76345171,\n",
       "        0.75055025, 0.71974641, 0.74643552, 0.73088059, 0.72035005,\n",
       "        0.75584795, 0.73185393]),\n",
       " 'split3_test_score': array([0.70754704, 0.72057741, 0.70501425, 0.70977208, 0.7111726 ,\n",
       "        0.70883379, 0.69926152, 0.71483877, 0.69889565, 0.70431593,\n",
       "        0.71300731, 0.70865429]),\n",
       " 'split4_test_score': array([0.68779326, 0.67770837, 0.67361564, 0.6771315 , 0.67346347,\n",
       "        0.68420529, 0.6786107 , 0.67066712, 0.67655039, 0.67011306,\n",
       "        0.6747077 , 0.67593025]),\n",
       " 'split5_test_score': array([0.71142698, 0.68746101, 0.69238453, 0.71850876, 0.69709652,\n",
       "        0.70286779, 0.70677258, 0.68883005, 0.68957256, 0.70794925,\n",
       "        0.68243141, 0.69102325]),\n",
       " 'split6_test_score': array([0.69851689, 0.68674832, 0.70171117, 0.66695157, 0.69598514,\n",
       "        0.69459891, 0.69172105, 0.67996408, 0.69060068, 0.67646724,\n",
       "        0.68592575, 0.69492933]),\n",
       " 'split7_test_score': array([0.67769158, 0.69598514, 0.69370287, 0.67001162, 0.70683216,\n",
       "        0.70154502, 0.67295175, 0.68770417, 0.68539214, 0.67442006,\n",
       "        0.70206521, 0.68655348]),\n",
       " 'split8_test_score': array([0.70804845, 0.73256512, 0.7161246 , 0.72648107, 0.72056811,\n",
       "        0.71227372, 0.69899057, 0.71686886, 0.70362722, 0.70311512,\n",
       "        0.72428605, 0.69583266]),\n",
       " 'split9_test_score': array([0.68609931, 0.71135358, 0.70113285, 0.69872973, 0.72086737,\n",
       "        0.70617618, 0.67365967, 0.70909965, 0.69932431, 0.68253968,\n",
       "        0.7159703 , 0.70556602]),\n",
       " 'mean_test_score': array([0.69512589, 0.70473528, 0.69833828, 0.69431308, 0.71001435,\n",
       "        0.7040872 , 0.68716921, 0.6984771 , 0.69154653, 0.68696708,\n",
       "        0.70308337, 0.69613838]),\n",
       " 'std_test_score': array([0.01713223, 0.02286375, 0.01946792, 0.02581512, 0.02365196,\n",
       "        0.02002316, 0.01831114, 0.02294142, 0.01859121, 0.01953299,\n",
       "        0.0256389 , 0.01695209]),\n",
       " 'rank_test_score': array([ 8,  2,  6,  9,  1,  3, 11,  5, 10, 12,  4,  7], dtype=int32),\n",
       " 'split0_train_score': array([0.99106589, 0.99975179, 0.99975179, 0.98480474, 0.99925513,\n",
       "        0.99975179, 0.991817  , 0.99975179, 0.99975179, 0.98707001,\n",
       "        0.99975179, 0.99975179]),\n",
       " 'split1_train_score': array([0.98884621, 0.99975182, 0.99975182, 0.98507269, 0.99925521,\n",
       "        0.99975182, 0.99083499, 0.99975182, 0.99975182, 0.98733452,\n",
       "        0.99950356, 0.99975182]),\n",
       " 'split2_train_score': array([0.99056451, 0.99975182, 0.99975182, 0.9863054 , 0.99975182,\n",
       "        0.99975182, 0.99181265, 0.99975182, 0.99975182, 0.98831268,\n",
       "        0.99975182, 0.99975182]),\n",
       " 'split3_train_score': array([0.99055838, 0.99975182, 0.99975182, 0.98427184, 0.99900679,\n",
       "        0.99975182, 0.99180735, 0.99975182, 0.99975182, 0.98730554,\n",
       "        0.99975182, 0.99975182]),\n",
       " 'split4_train_score': array([0.99081132, 1.        , 1.        , 0.98578395, 0.9995034 ,\n",
       "        1.        , 0.99230636, 1.        , 1.        , 0.98879533,\n",
       "        1.        , 1.        ]),\n",
       " 'split5_train_score': array([0.98858917, 0.99975182, 0.99975182, 0.98581673, 0.99900679,\n",
       "        0.99975182, 0.98983112, 0.99975182, 0.99975182, 0.9883203 ,\n",
       "        0.99975182, 0.99975182]),\n",
       " 'split6_train_score': array([0.99254609, 1.        , 1.        , 0.9872888 , 0.99975174,\n",
       "        1.        , 0.99279806, 1.        , 1.        , 0.99029566,\n",
       "        1.        , 1.        ]),\n",
       " 'split7_train_score': array([0.98982125, 0.99975182, 0.99975182, 0.98579336, 0.99925521,\n",
       "        0.99975182, 0.99082024, 0.99975182, 0.99975182, 0.98681205,\n",
       "        0.99975182, 0.99975182]),\n",
       " 'split8_train_score': array([0.98983226, 0.99975193, 0.99975193, 0.98579494, 0.99925554,\n",
       "        0.99975193, 0.99107654, 0.99975193, 0.99975193, 0.9883178 ,\n",
       "        0.99975193, 0.99975193]),\n",
       " 'split9_train_score': array([0.9905808 , 0.99975193, 0.99975193, 0.9853159 , 0.99950377,\n",
       "        0.99975193, 0.991575  , 0.99975193, 0.99975193, 0.98782862,\n",
       "        0.99975193, 0.99975193]),\n",
       " 'mean_train_score': array([0.99032159, 0.99980147, 0.99980147, 0.98562484, 0.99935454,\n",
       "        0.99980147, 0.99146793, 0.99980147, 0.99980147, 0.98803925,\n",
       "        0.99977665, 0.99980147]),\n",
       " 'std_train_score': array([1.07667411e-03, 9.92633143e-05, 9.92633143e-05, 7.90877591e-04,\n",
       "        2.53238869e-04, 9.92633143e-05, 8.05731148e-04, 9.92633143e-05,\n",
       "        9.92633143e-05, 9.70523696e-04, 1.33662916e-04, 9.92633143e-05])}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens. These tokens are often loosely referred to as terms or words, but it is sometimes important to make distinction.\n",
    "\n",
    "Usually, the punctuation is removed together with twitter specific tokens like |LBR| (new line).\n",
    "\n",
    "What should one do with the following examples?\n",
    "\n",
    "Jetzt habt ihr's schon wieder FAST geschafft\n",
    "\n",
    "ES IST NUR UND AUSSCHLIE√üLICH DER ISLAM, ALSO A L L E UND J E D E R MOSLEM\n",
    "\n",
    "Sicher doch und es w√ºrde besser gehen, wenn 100TSD Mecklenburger K√ºhe das politische Berlin zuschei√üen w√ºrden üòú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer as Tokenizer_NLTK\n",
    "from nltk.tokenize.casual import remove_handles\n",
    "from nltk.stem.snowball import GermanStemmer as Stemmer_NLTK\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, preserve_case=True, use_stemmer=False, join=False):\n",
    "        self.preserve_case=preserve_case\n",
    "        self.use_stemmer=use_stemmer\n",
    "        self.join=join\n",
    "\n",
    "    def tokenize(self, tweet):\n",
    "        tweet=remove_handles(tweet)\n",
    "        tweet=tweet.replace('#', ' ')\n",
    "        tweet=tweet.replace('&lt;', ' ')\n",
    "        tweet=tweet.replace('&gt;', ' ')\n",
    "        tweet=tweet.replace('&amp;', ' und ')\n",
    "        tweet=tweet.replace('|LBR|', ' ')\n",
    "        tweet=tweet.replace('-', ' ')\n",
    "        tweet=tweet.replace('_', ' ')\n",
    "        tweet=tweet.replace(\"'s\", ' ')\n",
    "        tweet=tweet.replace(\",\", ' ')\n",
    "        tweet=tweet.replace(\";\", ' ')\n",
    "        tweet=tweet.replace(\":\", ' ')\n",
    "        tweet=tweet.replace(\"/\", ' ')\n",
    "        tweet=tweet.replace(\"+\", ' ')\n",
    "        tknzr=Tokenizer_NLTK(preserve_case=self.preserve_case, reduce_len=True)\n",
    "\n",
    "        if self.join:\n",
    "            return \" \".join(tknzr.tokenize(tweet))\n",
    "        elif self.use_stemmer:\n",
    "            stmmr=Stemmer_NLTK()\n",
    "            return [stmmr.stem(token) for token in tknzr.tokenize(tweet)]\n",
    "        else:\n",
    "            return tknzr.tokenize(tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lauf, gelaufen, lauffend, l√§uft but also Demokratie, demokratisch und Demokratisierung. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. The difference is in the fact, that stemming usually refers to a crude heuristic process that chops off the beginnings and ends of words. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma \n",
    "\n",
    "Both of them would succeed in transforming the word gegessen to essen, but only the Lematizer would convert \"ich a√ü\" to ich essen, the stemmer would not perform any transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German specific challenge - compound splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Austrian Word of the Year 2016 was Bundespr√§sidentenstichwahlwiederholungsverschiebung. An iterative process of finding the longest part of the word in the dictionary and splitting can be applied. The result could be:\n",
    "Bundespr√§sident Stichwahl Wiederholung Verschiebung. How is it with \"Bundespr√§sident\" vs \"Bund Pr√§sident\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "token_vect=TfidfVectorizer(analyzer=\"word\", max_df=0.01, min_df=0.0002,\n",
    "                             tokenizer=Tokenizer(preserve_case=False, use_stemmer=True).tokenize)\n",
    "\n",
    "char_vect  = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 7), max_df=0.01, min_df=0.0002,\n",
    "                             preprocessor=Tokenizer(preserve_case=False, join=True).tokenize)\n",
    "# %%\n",
    "X_TNGR_train = token_vect.fit_transform(X_train)\n",
    "X_TNGR_test  = token_vect.transform(X_test)\n",
    "\n",
    "X_CNGR_train = char_vect.fit_transform(X_train)\n",
    "X_CNGR_test  = char_vect.transform(X_test)\n",
    "#%%\n",
    "\n",
    "#%%\n",
    "text_clf = Pipeline([\n",
    "    ('vect', token_vect),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "parameters={\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "    'vect__use_idf': (True, False),\n",
    "    'clf__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "gs_clf=GridSearchCV(text_clf, parameters, cv=StratifiedKFold(n_splits=10), iid=False,\n",
    "                      n_jobs=-1, scoring=make_scorer(f1_score, average='macro'))\n",
    "gs_clf=gs_clf.fit(X_train, Y_train1)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "\n",
    "# %%\n",
    "gs_clf.best_score_\n",
    "\n",
    "# %%\n",
    "text_clf.set_params(**gs_clf.best_params_)\n",
    "text_clf.fit(X_train, Y_train1)\n",
    "predicted=text_clf.predict(X_test)\n",
    "np.mean(predicted == Y_test1)\n",
    "# %%\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(predicted, Y_test1, average='macro')\n",
    "# %%\n",
    "token_vect=TfidfVectorizer(analyzer=\"word\", max_df=0.01, min_df=0.0002,\n",
    "                             tokenizer=Tokenizer(preserve_case=False, use_stemmer=True).tokenize)\n",
    "\n",
    "char_vect  = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 7), max_df=0.01, min_df=0.0002,\n",
    "                             preprocessor=Tokenizer(preserve_case=False, join=True).tokenize)\n",
    "# %%\n",
    "\n",
    "X_TNGR_train = token_vect.fit_transform(X_train)\n",
    "X_TNGR_test  = token_vect.transform(X_test)\n",
    "\n",
    "X_CNGR_train = char_vect.fit_transform(X_train)\n",
    "X_CNGR_test  = char_vect.transform(X_test)\n",
    "\n",
    "#%%\n",
    "def get_META_feats(clf, X_train, X_test, y, seeds=[42]):\n",
    "    feats_train = []\n",
    "    for seed in seeds:\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "        feats_train.append(cross_val_predict(clf, X_train, y=y, method='predict_proba', cv=skf, n_jobs=-1))\n",
    "    feats_train = np.mean(feats_train, axis=0)\n",
    "    \n",
    "    clf.fit(X_train, y)\n",
    "    feats_test = clf.predict_proba(X_test)\n",
    "    \n",
    "    return feats_train, feats_test\n",
    "\n",
    "#%%\n",
    "clfs_task1 = [LogisticRegression(class_weight='balanced'),\n",
    "              ExtraTreesClassifier(n_estimators=100, criterion='entropy', n_jobs=-1),\n",
    "              ExtraTreesClassifier(n_estimators=100, criterion='gini', n_jobs=-1)]\n",
    "\n",
    "base_feats_task1 = [#(X_CNGR_train, X_CNGR_test),\n",
    "                    (X_TNGR_train, X_TNGR_test)]\n",
    "\n",
    "X_META_task1_train = []\n",
    "X_META_task1_test  = []\n",
    "for X_train, X_test in base_feats_task1:\n",
    "    for clf in clfs_task1:\n",
    "        feats = get_META_feats(clf, X_train, X_test, Y_train1)\n",
    "        X_META_task1_train.append(feats[0])\n",
    "        X_META_task1_test.append(feats[1])\n",
    "        \n",
    "X_META_task1_train = np.concatenate(X_META_task1_train, axis=1)\n",
    "X_META_task1_test  = np.concatenate(X_META_task1_test, axis=1)\n",
    "\n",
    "\n",
    "#%%\n",
    "clf_task1 = LogisticRegression(C=0.17, class_weight='balanced')\n",
    "clf_task1.fit(X_META_task1_train, Y_train1)\n",
    "\n",
    "preds_task1 = clf_task1.predict(X_META_task1_test)    \n",
    "\n",
    "\n",
    "#%%\n",
    "np.mean(preds_task1 == Y_test1)\n",
    "# %%\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(preds_task1, Y_test1, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary of bad words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T09:58:21.055904Z",
     "start_time": "2019-05-09T09:58:21.051346Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./data/lexicon.txt\", \"r\") as text_file:\n",
    "    lexicon = text_file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T09:58:21.717005Z",
     "start_time": "2019-05-09T09:58:21.701804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAA Batterie',\n",
       " 'ABS-Bremser',\n",
       " 'Aa Esser',\n",
       " 'Aa Fresser',\n",
       " 'Aa Gesicht',\n",
       " 'Aa Lecker',\n",
       " 'Aa Loch',\n",
       " 'Aa Lutscher',\n",
       " 'Aa Wurst',\n",
       " 'Aal',\n",
       " 'Aalauge',\n",
       " 'Aalficker',\n",
       " 'Aalfresse',\n",
       " 'Aalschwanz',\n",
       " 'Aalwurstverk√§ufer',\n",
       " 'Aas',\n",
       " 'Aasaffe',\n",
       " 'Aasfresser',\n",
       " 'Aasgeier',\n",
       " 'Abart',\n",
       " 'Abbumser',\n",
       " 'Abdeckstiftbenutzer',\n",
       " 'Abdeckstiftdauerbenutzer',\n",
       " 'Abdomen',\n",
       " 'Abfall',\n",
       " 'Abfall, biochemischer',\n",
       " 'Abfallecker',\n",
       " 'Abfalleimervagina',\n",
       " 'Abfallficker',\n",
       " 'Abfallproduckt',\n",
       " 'Abfallprodukt',\n",
       " 'Abfallschlucker',\n",
       " 'Abfalltonnenvollschei√üer',\n",
       " 'Abficker',\n",
       " 'Abflussrohrgucker',\n",
       " 'Abflussrohrsauger',\n",
       " 'Abflu√üverstopfer',\n",
       " 'Abgard',\n",
       " 'Abgaskakerlake',\n",
       " 'Abgaslaus',\n",
       " 'Abgasproduzent',\n",
       " 'Abgefickter',\n",
       " 'Abnippeler',\n",
       " 'Abort',\n",
       " 'Abortdeckel',\n",
       " 'Abortsch√ºsseltaucher',\n",
       " 'Abschaum',\n",
       " 'Abschei√üer',\n",
       " 'Abschiedswinker',\n",
       " 'Abseiler',\n",
       " 'Abseitserkl√§rer',\n",
       " 'Abspritzer',\n",
       " 'Abspritzmuschi',\n",
       " 'Abspritzmuschie',\n",
       " 'Absturztorte',\n",
       " 'Absturzvogel',\n",
       " 'Abszess',\n",
       " 'Abt√∂rner',\n",
       " 'Abwasserschl√ºfer',\n",
       " 'Abwasserschl√ºrfer',\n",
       " 'Abwasserspucker',\n",
       " 'Abwasserverpester',\n",
       " 'Abwichhure',\n",
       " 'Abwichshure',\n",
       " 'Abzocker',\n",
       " 'Achselbaron',\n",
       " 'Achselfasching',\n",
       " 'Achselficker',\n",
       " 'Achselgrind',\n",
       " 'Achselhaarflechter',\n",
       " 'Achselhaarfris√∂r',\n",
       " 'Achselhaarf√∂ner',\n",
       " 'Achselhaarlutscher',\n",
       " 'Achselhaarratte',\n",
       " 'Achselhaarschneider',\n",
       " 'Achselhaarspalter',\n",
       " 'Achselh√∂hlenforscher',\n",
       " 'Achsellaus',\n",
       " 'Achsellecker',\n",
       " 'Achselriecher',\n",
       " 'Achselschwei√üfontaine',\n",
       " 'Achsenh√∂hlenforscher',\n",
       " 'Achterbahnindermittesitzer',\n",
       " 'Achterbahnkotzer',\n",
       " 'Achtstundenschl√§fer',\n",
       " 'Ackerarsch',\n",
       " 'Ackerfresse',\n",
       " 'Ackerpilot',\n",
       " 'Admiral Anal',\n",
       " 'Affe',\n",
       " 'Affe, abgeleckter',\n",
       " 'Affe, haarloser',\n",
       " 'Affenafterlecker',\n",
       " 'Affenarsch',\n",
       " 'Affenarschfetischist',\n",
       " 'Affenarschficker',\n",
       " 'Affenarschfurchenfetischist',\n",
       " 'Affenarschgesicht',\n",
       " 'Affenarschkratzer',\n",
       " 'Affenarschlecker',\n",
       " 'Affenarschloch',\n",
       " 'Affenarschpickel',\n",
       " 'Affenarschpopper',\n",
       " 'Affenarschritzenlecker',\n",
       " 'Affenarschwichser',\n",
       " 'Affenbesamer',\n",
       " 'Affenbumser',\n",
       " 'Affeneichellecker',\n",
       " 'Affenfehlfick',\n",
       " 'Affenficker',\n",
       " 'Affenfotze',\n",
       " 'Affenfresse',\n",
       " 'Affenfurchenfetischist',\n",
       " 'Affengesicht',\n",
       " 'Affenhausgucker',\n",
       " 'Affenhirn',\n",
       " 'Affenhirnschei√üer',\n",
       " 'Affenhirsch',\n",
       " 'Affenhoden',\n",
       " 'Affenjockel',\n",
       " 'Affenjunge',\n",
       " 'Affenkacke',\n",
       " 'Affenkimme',\n",
       " 'Affenkind',\n",
       " 'Affenklaus',\n",
       " 'Affenkopf',\n",
       " 'Affenkot',\n",
       " 'Affenk√∂nig',\n",
       " 'Affenlecker',\n",
       " 'Affenlutscher',\n",
       " 'Affenmann',\n",
       " 'Affenmaul',\n",
       " 'Affenmensch',\n",
       " 'Affenmuschilecker',\n",
       " 'Affenmutantenschiss',\n",
       " 'Affenpenis',\n",
       " 'Affenpenissauger',\n",
       " 'Affenpimmel',\n",
       " 'Affenpopo',\n",
       " 'Affenprinz',\n",
       " 'Affenpupser',\n",
       " 'Affenreiter',\n",
       " 'Affensack',\n",
       " 'Affenschaukel',\n",
       " 'Affenscheidensekretschlecker',\n",
       " 'Affenschei√üe',\n",
       " 'Affenschwanzlutscher',\n",
       " 'Affentitte',\n",
       " 'Affenvergewaltiger',\n",
       " 'Affenvisage',\n",
       " 'Affenwesen',\n",
       " 'Affenwichser',\n",
       " 'Affenwichsgesicht',\n",
       " 'Affenzicke',\n",
       " 'Affgo',\n",
       " 'Affipups',\n",
       " 'Affupeter',\n",
       " 'Afterabschmatzer',\n",
       " 'Afteratmer',\n",
       " 'Afterforscher',\n",
       " 'Aftergei√üel',\n",
       " 'Aftergelehrter',\n",
       " 'Afterh√∂hlenbewohner',\n",
       " 'Afterh√∂hlenforscher',\n",
       " 'Afterh√∂lenforscher',\n",
       " 'Afterh√∂leninspektor',\n",
       " 'Afterh√∂llenfrosch',\n",
       " 'Afterkopf',\n",
       " 'Afterkriecher',\n",
       " 'Afterkritiker',\n",
       " 'Afterlecker',\n",
       " 'Afterloser',\n",
       " 'Afterschleim',\n",
       " 'Afterschleimhautfalte',\n",
       " 'Afterschlemmerer',\n",
       " 'Afterschl√ºrfer',\n",
       " 'Afterschmatzer',\n",
       " 'Afterst√∂psel',\n",
       " 'Agrobolzen',\n",
       " 'Airbagnachr√ºster',\n",
       " 'Akkuschrauber',\n",
       " 'Aktentaschentr√§ger',\n",
       " 'Aktienfr√ºhverk√§ufer',\n",
       " 'Alf',\n",
       " 'Algengr√ºtze',\n",
       " 'Algenkotzer',\n",
       " 'Algenpillenschlucker',\n",
       " 'Alien',\n",
       " 'Alienmanscher',\n",
       " 'Alki',\n",
       " 'Alkoholiker',\n",
       " 'Alkoholkl√§rwerk',\n",
       " 'Allerweltshure',\n",
       " 'Allerweltsnutte',\n",
       " 'Alles zwei mal frager',\n",
       " 'Allesbegatter',\n",
       " 'Allesbesserwisser',\n",
       " 'Allesgleicherlediger',\n",
       " 'Allesint√ºteneinpacker',\n",
       " 'Almdudler',\n",
       " 'Alpenfutt',\n",
       " 'Alphornsauger',\n",
       " 'Alptraum',\n",
       " 'Alptr√§umer',\n",
       " 'Altenheimhure',\n",
       " 'Altenheimnutte',\n",
       " 'Altenheimschlampe',\n",
       " 'Altersheimbesucher',\n",
       " 'Altpapiersammler',\n",
       " 'Altreifensammler',\n",
       " 'Altsalami',\n",
       " 'Altweiberficker',\n",
       " 'Alufolienglattstreicher',\n",
       " 'Alufoliengriller',\n",
       " 'Amaisenb√§r',\n",
       " 'Amalgamverweigerer',\n",
       " 'Amateur',\n",
       " 'Amatuerversager',\n",
       " 'Ameisenficker',\n",
       " 'Ameisenfotze',\n",
       " 'Ameisenfris√∂r',\n",
       " 'Ampelanhalter',\n",
       " 'Ampeldr√ºcker',\n",
       " 'Ampelgelbbremser',\n",
       " 'Ampelranroller',\n",
       " 'Ampel√ºberseher',\n",
       " 'Amsel',\n",
       " 'Am√∂be',\n",
       " 'Am√∂benbefruchter',\n",
       " 'Am√∂benliebhaber',\n",
       " 'Am√∂benverf√ºhrer',\n",
       " 'Anabolikafratze',\n",
       " 'Anabolikafresse',\n",
       " 'Analacrobat',\n",
       " 'Analadmiral',\n",
       " 'Analakrobat',\n",
       " 'Analakustiker',\n",
       " 'Analanalytiker',\n",
       " 'Analangler',\n",
       " 'Analansaugstutzen',\n",
       " 'Analapfelficker',\n",
       " 'Analarbeiter',\n",
       " 'Analarsch',\n",
       " 'Analassistent',\n",
       " 'Analatmer',\n",
       " 'Analauflauf',\n",
       " 'Analausfluss',\n",
       " 'Analauslauf',\n",
       " 'Analball',\n",
       " 'Analbanana',\n",
       " 'Analbaron',\n",
       " 'Analbarthaber',\n",
       " 'Analbefriedigungsger√§t',\n",
       " 'Analbefruchter',\n",
       " 'Analbegutachter',\n",
       " 'Analbergsteiger',\n",
       " 'Analbergteiger',\n",
       " 'Analbesamer',\n",
       " 'Analbesen',\n",
       " 'Analbimbo',\n",
       " 'Analbitch',\n",
       " 'Analbohrer',\n",
       " 'Analbrause',\n",
       " 'Analbremse',\n",
       " 'Analbrot',\n",
       " 'Analbruder',\n",
       " 'Analbumser',\n",
       " 'Analb√§r',\n",
       " 'Analb√ºrste',\n",
       " 'Analcharakter',\n",
       " 'Analcowboy',\n",
       " 'Analdackel',\n",
       " 'Analdehner',\n",
       " 'Analdiener',\n",
       " 'Analdildo',\n",
       " 'Analdildolutscher',\n",
       " 'Analdruide',\n",
       " 'Analdr√ºsenausquetscher',\n",
       " 'Analdr√ºsenfetish',\n",
       " 'Analdr√ºsensaftlecker',\n",
       " 'Analdr√ºsenstinker',\n",
       " 'Analdusche',\n",
       " 'Analduscher',\n",
       " 'Analeber',\n",
       " 'Analeingangsausleuchter',\n",
       " 'Analente',\n",
       " 'Analereignisverachter',\n",
       " 'Analeroberer',\n",
       " 'Analerotiker',\n",
       " 'Analfan',\n",
       " 'Analfee',\n",
       " 'Analfetischist',\n",
       " 'Analfetischistensau',\n",
       " 'Analficker',\n",
       " 'Analfickfehler',\n",
       " 'Analfickung',\n",
       " 'Analfighter',\n",
       " 'Analfisch',\n",
       " 'Analfissur',\n",
       " 'Analfistel',\n",
       " 'Analfister',\n",
       " 'Analfitischist',\n",
       " 'Analflokati',\n",
       " 'Analflummi',\n",
       " 'Analflunder',\n",
       " 'Analflutscher',\n",
       " 'Analfl√∂tenspieler',\n",
       " 'Analfl√∂ter',\n",
       " 'Analforscher',\n",
       " 'Analfotze',\n",
       " 'Analfotzenficker',\n",
       " 'Analfotzenlecker',\n",
       " 'Analfotzenschlecker',\n",
       " 'Analfotzentieftaucher',\n",
       " 'Analfotzenvergewaltiger',\n",
       " 'Analfraktur',\n",
       " 'Analfratze',\n",
       " 'Analfregatte',\n",
       " 'Analfresse',\n",
       " 'Analfrikadelle',\n",
       " 'Analfrikt√∂se',\n",
       " 'Analfris√∂se',\n",
       " 'Analfrosch',\n",
       " 'Analfrucht',\n",
       " 'Analfuchtel',\n",
       " 'Analfummmler',\n",
       " 'Analfurchenfetischist',\n",
       " 'Analf√∂rster',\n",
       " 'Analf√∂tus',\n",
       " 'Analf√ºrst',\n",
       " 'Analgalster',\n",
       " 'Analgaukler',\n",
       " 'Analgeburt',\n",
       " 'Analgeier',\n",
       " 'Analgeige',\n",
       " 'Analgeneral',\n",
       " 'Analgeruchbel√§stiger',\n",
       " 'Analgeschwader',\n",
       " 'Analgesicht',\n",
       " 'Analgewitter',\n",
       " 'Analgezeugter',\n",
       " 'Analgiraffe',\n",
       " 'Analglocke',\n",
       " 'Analgnom',\n",
       " 'Analgoldfisch',\n",
       " 'Analgorn',\n",
       " 'Analgourmet',\n",
       " 'Analgrabscher',\n",
       " 'Analgranate',\n",
       " 'Analgrotte',\n",
       " 'Analgurke',\n",
       " 'Analhaark√§mmer',\n",
       " 'Analhackfresse',\n",
       " 'Analheld',\n",
       " 'Analhelfer',\n",
       " 'Analhengst',\n",
       " 'Analhero',\n",
       " 'Analhexe',\n",
       " 'Analhoden',\n",
       " 'Analhodenbeschw√∂rer',\n",
       " 'Analhopper',\n",
       " 'Analhorn',\n",
       " 'Analhorst',\n",
       " 'Analhuber',\n",
       " 'Analhund',\n",
       " 'Analhupe',\n",
       " 'Analhure',\n",
       " 'Analhurengrabscher',\n",
       " 'Analhurrikan',\n",
       " 'Analhusten',\n",
       " 'Analhuster',\n",
       " 'Analhyperbel',\n",
       " 'Analh√∂hlenforscher',\n",
       " 'Analh√∂lenbefruchter',\n",
       " 'Analh√ºpfer',\n",
       " 'Anali',\n",
       " 'Analintigrator',\n",
       " 'Analintruder',\n",
       " 'Analist',\n",
       " 'Analjunge',\n",
       " 'Analkaiser',\n",
       " 'Analkaktus',\n",
       " 'Analkanone',\n",
       " 'Analkasper',\n",
       " 'Analkatzenficker',\n",
       " 'Analkeks',\n",
       " 'Analkerze',\n",
       " 'Analkeule',\n",
       " 'Analkind',\n",
       " 'Analkingdome',\n",
       " 'Analklabauter',\n",
       " 'Analkleckerburg',\n",
       " 'Analklemme',\n",
       " 'Analklemtner',\n",
       " 'Analkloake',\n",
       " 'Analknecht',\n",
       " 'Analkn√∂del',\n",
       " 'Analkobold',\n",
       " 'Analkonfekt',\n",
       " 'Analkopf',\n",
       " 'Analkorken',\n",
       " 'Analkotzer',\n",
       " 'Analkrabbe',\n",
       " 'Analkracher',\n",
       " 'Analkreatur',\n",
       " 'Analkrebs',\n",
       " 'Analkricher',\n",
       " 'Analkriecher',\n",
       " 'Analkrieger',\n",
       " 'Analkrokette',\n",
       " 'Analkr√∂te',\n",
       " 'Analk√§fer',\n",
       " 'Analk√§se',\n",
       " 'Analk√∂nig',\n",
       " 'Analk√ºrbis',\n",
       " 'Anallampe',\n",
       " 'Anallecker',\n",
       " 'Analliebhaber',\n",
       " 'Analligator',\n",
       " 'Anallocke',\n",
       " 'Analloge',\n",
       " 'Analluftpumpe',\n",
       " 'Anallunte',\n",
       " 'Anallutscher',\n",
       " 'Analmade',\n",
       " 'Analmaler',\n",
       " 'Analmassage',\n",
       " 'Analmasturbierer',\n",
       " 'Analmissgeburt',\n",
       " 'Analnasenb√§r',\n",
       " 'Analnutte',\n",
       " 'Analofant',\n",
       " 'Analonanierer',\n",
       " 'Analorgel',\n",
       " 'Analotter',\n",
       " 'Analpelz',\n",
       " 'Analpenis',\n",
       " 'Analpenner',\n",
       " 'Analperlentaucher',\n",
       " 'Analpfahl',\n",
       " 'Analpfannkuchen',\n",
       " 'Analpfeife',\n",
       " 'Analpfeifenputzer',\n",
       " 'Analpfirsich',\n",
       " 'Analpfosten',\n",
       " 'Analpfropf',\n",
       " 'Analphabet',\n",
       " 'Analpilger',\n",
       " 'Analpilot',\n",
       " 'Analpirat',\n",
       " 'Analpisser',\n",
       " 'Analpizza',\n",
       " 'Analpolierer',\n",
       " 'Analpopulist',\n",
       " 'Analpraktikant',\n",
       " 'Analpriester',\n",
       " 'Analprinz',\n",
       " 'Analprinzessin',\n",
       " 'Analprobe',\n",
       " 'Analprodukt',\n",
       " 'Analprolaps',\n",
       " 'Analprolet',\n",
       " 'Analpropeller',\n",
       " 'Analpumpe',\n",
       " 'Analpupser',\n",
       " 'Analqueen',\n",
       " 'Analquetsche',\n",
       " 'Analrabauke',\n",
       " 'Analrakete',\n",
       " 'Analrasierer',\n",
       " 'Analratte',\n",
       " 'Analraupe',\n",
       " 'Analreiniger',\n",
       " 'Analreinigungskraft',\n",
       " 'Analreiter',\n",
       " 'Analritter',\n",
       " 'Analrohrgeburt',\n",
       " 'Analrolle',\n",
       " 'Analrosine',\n",
       " 'Analruine',\n",
       " 'Analrutsche',\n",
       " 'Analrutschfan',\n",
       " 'Analr√§uber',\n",
       " 'Analr√ºlpser',\n",
       " 'Analsafttrinker',\n",
       " 'Analsauficker',\n",
       " 'Analschakal',\n",
       " 'Analschimmel',\n",
       " 'Analschlecker',\n",
       " 'Analschleimtrinker',\n",
       " 'Analschlemmer',\n",
       " 'Analschleuder',\n",
       " 'Analschl√ºrfer',\n",
       " 'Analschuster',\n",
       " 'Analsch√§del',\n",
       " 'Analsch√ºler',\n",
       " 'Analseuche',\n",
       " 'Analseufzer',\n",
       " 'Analsexritter',\n",
       " 'Analsonde',\n",
       " 'Analsondierer',\n",
       " 'Analso√üenschl√ºrfer',\n",
       " 'Analspasti',\n",
       " 'Analspecht',\n",
       " 'Analspekulum',\n",
       " 'Analspender',\n",
       " 'Analspezialist',\n",
       " 'Analspiegelbild',\n",
       " 'Analspreizer',\n",
       " 'Analspritzer',\n",
       " 'Analstange',\n",
       " 'Analstecher',\n",
       " 'Analstopfer',\n",
       " 'Analstrecker',\n",
       " 'Analst√∂psel',\n",
       " 'Analst√∂psellutscher',\n",
       " 'Analst√∂pseltr√§ger',\n",
       " 'Analst√∂psler',\n",
       " 'Anals√§ufer',\n",
       " 'Anals√ºchtiger',\n",
       " 'Analtamponfresser',\n",
       " 'Analtankstelle',\n",
       " 'Analtaucher',\n",
       " 'Analtelefonist',\n",
       " 'Analterminator',\n",
       " 'Analterrorist',\n",
       " 'Analtieftaucher',\n",
       " 'Analtittenbumser',\n",
       " 'Analtorpedo',\n",
       " 'Analtorte',\n",
       " 'Analtranse',\n",
       " 'Analtransvestit',\n",
       " 'Analtrichter',\n",
       " 'Analtrompete',\n",
       " 'Analtrompetenflachwichser',\n",
       " 'Analtrumpete',\n",
       " 'Analt√ºtenventilator',\n",
       " 'Analvagina',\n",
       " 'Analventilator',\n",
       " 'Analverfechter',\n",
       " 'Analverkehrsbetreiber',\n",
       " 'Analvernichter',\n",
       " 'Analverteiler',\n",
       " 'Analvertilger',\n",
       " 'Analverw√∂hner',\n",
       " 'Analvibrator',\n",
       " 'Analvinoline',\n",
       " 'Analvioline',\n",
       " 'Analwanze',\n",
       " 'Analwasserlecker',\n",
       " 'Analweiterrungsger√§t',\n",
       " 'Analwichkopf',\n",
       " 'Analwichser',\n",
       " 'Analwind',\n",
       " 'Analwissenschaftler',\n",
       " 'Analwurm',\n",
       " 'Analwurst',\n",
       " 'Analytiker',\n",
       " 'Analzapfen',\n",
       " 'Analzerfetzer',\n",
       " 'Analzerleger',\n",
       " 'Analzwicker',\n",
       " 'Analzyklop',\n",
       " 'Analzylinder',\n",
       " 'Anarchist',\n",
       " 'Anarchopsychopath',\n",
       " 'Anbaggerer',\n",
       " 'Anbl√§ser',\n",
       " 'Andenlama',\n",
       " 'Anf√§nger',\n",
       " 'Angeber',\n",
       " 'Angelrute',\n",
       " 'Angelrutenknutscher',\n",
       " 'Angelutschter',\n",
       " 'Angoraw√§schetr√§ger',\n",
       " 'Angsthase',\n",
       " 'Angststinker',\n",
       " 'Anh√§ngerkupplungsabdecker',\n",
       " 'Anmacher',\n",
       " 'Anonymspender',\n",
       " 'Anrufbeantworterspruchwechsler',\n",
       " 'Anstaltsschlampe',\n",
       " 'Anstandsrestelasser',\n",
       " 'Anstandsrest√ºbriglasser',\n",
       " 'Antianthropologe',\n",
       " 'Antibiotikaschlucker',\n",
       " 'Antifaltencremebenutzer',\n",
       " 'Antiheld',\n",
       " 'Antihyperhero',\n",
       " 'Antiindividuum',\n",
       " 'Antilope',\n",
       " 'Antilopenarsch',\n",
       " 'Antimensch',\n",
       " 'Antiperson',\n",
       " 'Antist√ºck',\n",
       " 'Antivirenprogrammupdater',\n",
       " 'Anus',\n",
       " 'Anus Analus',\n",
       " 'Anusauge',\n",
       " 'Anusbefriediger',\n",
       " 'Anusberater',\n",
       " 'Anusbewunderer',\n",
       " 'Anusbiber',\n",
       " 'Anusb√§rchen',\n",
       " 'Anusdildo',\n",
       " 'Anusdoktor',\n",
       " 'Anusexperte',\n",
       " 'Anusficker',\n",
       " 'Anusforscher',\n",
       " 'Anusfresse',\n",
       " 'Anusfrikadelle',\n",
       " 'Anusgeburt',\n",
       " 'Anusgefisteter',\n",
       " 'Anuskitzelfinger',\n",
       " 'Anuskopf',\n",
       " 'Anuskrapfen',\n",
       " 'Anuslecker',\n",
       " 'Anusloch',\n",
       " 'Anusl√∂ffler',\n",
       " 'Anusmonster',\n",
       " 'Anuspenetrierer',\n",
       " 'Anuspf√∂rtner',\n",
       " 'Anusranger',\n",
       " 'Anusriecher',\n",
       " 'Anusverengungsmaschine',\n",
       " 'Anwesender',\n",
       " 'Apfel, fauler',\n",
       " 'Apfelarsch',\n",
       " 'Apfelbacke',\n",
       " 'Apfelbaumbumser',\n",
       " 'Apfelfotze',\n",
       " 'Apfelkernficker',\n",
       " 'Apfelkuchen',\n",
       " 'Apfelkuchenficker',\n",
       " 'Apfellecker',\n",
       " 'Apfelschlampe',\n",
       " 'Apfelschorlenschlucker',\n",
       " 'Apfeltasche',\n",
       " 'Apflussficker',\n",
       " 'Apflussfresser',\n",
       " 'Apflusslecker',\n",
       " 'Apflusssammler',\n",
       " 'Apothekeng√§nger',\n",
       " 'Arbeitsbeschaffungsma√ünahme',\n",
       " 'Arbeitsscheuer',\n",
       " 'Arbeitszeiterfasser',\n",
       " 'Archgewichster',\n",
       " 'Archpickel',\n",
       " 'Armbanduhrtr√§ger',\n",
       " 'Armleuchter',\n",
       " 'Armleuchterhalter',\n",
       " 'Armseliger',\n",
       " 'Arnalreiter',\n",
       " 'Arnalritter',\n",
       " 'Arnuskatoffelbauer',\n",
       " 'Arsch',\n",
       " 'Arsch mit Ohren',\n",
       " 'Arsch mit Poposchinken',\n",
       " 'Arsch offen Haber',\n",
       " 'Arschabfall',\n",
       " 'Arschablecker',\n",
       " 'Arschabputzer',\n",
       " 'Arschaffe',\n",
       " 'Arschantilope',\n",
       " 'Arschaufrei√üer',\n",
       " 'Arschauskratzer',\n",
       " 'Arschbacke',\n",
       " 'Arschbackenj√§ger',\n",
       " 'Arschbackenkasper',\n",
       " 'Arschbackenkind',\n",
       " 'Arschbackenklatscher',\n",
       " 'Arschbackenspreizer',\n",
       " 'Arschbackentoni',\n",
       " 'Arschbanane',\n",
       " 'Arschbefeuchter',\n",
       " 'Arschbegatter',\n",
       " 'Arschbeleuchtung',\n",
       " 'Arschbepisser',\n",
       " 'Arschbesamer',\n",
       " 'Arschbesamungmaschine',\n",
       " 'Arschbimbomassierer',\n",
       " 'Arschbirne',\n",
       " 'Arschbock',\n",
       " 'Arschbohrer',\n",
       " 'Arschbohrmaschine',\n",
       " 'Arschbolzen',\n",
       " 'Arschbombe',\n",
       " 'Arschbratze',\n",
       " 'Arschbremse',\n",
       " 'Arschbumser',\n",
       " 'Arschbuse',\n",
       " 'Arschb√§r',\n",
       " 'Arschb√ºrste',\n",
       " 'Arschcham√§leon',\n",
       " 'Arschclown',\n",
       " 'Arschdepp',\n",
       " 'Arschdrenage',\n",
       " 'Arscheinficker',\n",
       " 'Arschensafter',\n",
       " 'Arschentjungferer',\n",
       " 'Arscheule',\n",
       " 'Arschfalte',\n",
       " 'Arschfaltenficker',\n",
       " 'Arschfaltenveredler',\n",
       " 'Arschfeigenfresse',\n",
       " 'Arschfetischist',\n",
       " 'Arschfickbombe',\n",
       " 'Arschfickemeister',\n",
       " 'Arschficker',\n",
       " 'Arschfickerhure',\n",
       " 'Arschfickerk√∂nig',\n",
       " 'Arschfickerlehrling',\n",
       " 'Arschfickerleisten',\n",
       " 'Arschfickersau',\n",
       " 'Arschfickfehler',\n",
       " 'Arschfickfehlgeburt',\n",
       " 'Arschfickfotze',\n",
       " 'Arschfickfotzenknecht',\n",
       " 'Arschfickgesicht',\n",
       " 'Arschfickk√∂nig',\n",
       " 'Arschficklecker',\n",
       " 'Arschficklehrling',\n",
       " 'Arschficknarbenlecker',\n",
       " 'Arschficknudel',\n",
       " 'Arschficknutte',\n",
       " 'Arschficksau',\n",
       " 'Arschfickschlampe',\n",
       " 'Arschficksohn',\n",
       " 'Arschfickweltmeister',\n",
       " 'Arschfickwichsnase',\n",
       " 'Arschfickziege',\n",
       " 'Arschfingerstecker',\n",
       " 'Arschflugzeug',\n",
       " 'Arschfl√∂te',\n",
       " 'Arschfonse',\n",
       " 'Arschfotze',\n",
       " 'Arschfotzenbremse',\n",
       " 'Arschfotzenficker',\n",
       " 'Arschfotzengedehnter',\n",
       " 'Arschfotzengesicht',\n",
       " 'Arschfotzenhirn',\n",
       " 'Arschfotzenlecker',\n",
       " 'Arschfotzenlutscher',\n",
       " 'Arschfotzenreiter',\n",
       " 'Arschfotzenzabel',\n",
       " 'Arschfotzte',\n",
       " 'Arschfresse',\n",
       " 'Arschfressenficker',\n",
       " 'Arschfresser',\n",
       " 'Arschfrisur',\n",
       " 'Arschfurchenausschlecker',\n",
       " 'Arschfurchenlecker',\n",
       " 'Arschfurunkel',\n",
       " 'Arschf√º√üler',\n",
       " 'Arschgaudi',\n",
       " 'Arschgebl√§se',\n",
       " 'Arschgebumsterhurensohn',\n",
       " 'Arschgeburt',\n",
       " 'Arschgefickter',\n",
       " 'Arschgeier',\n",
       " 'Arschgeige',\n",
       " 'Arschgeige, mit Arschhaaren bespannte',\n",
       " 'Arschgeigenbefruchter',\n",
       " 'Arschgeigenbewunderer',\n",
       " 'Arschgeigenfrontalwichser',\n",
       " 'Arschgeigengesicht',\n",
       " 'Arschgeigenhansel',\n",
       " 'Arschgeigenkuchenwicher',\n",
       " 'Arschgeigenlutscher',\n",
       " 'Arschgeigensau',\n",
       " 'Arschgeigenspieler',\n",
       " 'Arschgeigentrompete',\n",
       " 'Arschgeiger',\n",
       " 'Arschgeleckter',\n",
       " 'Arschgelekterhurensohn',\n",
       " 'Arschgelochter',\n",
       " 'Arschgeneral',\n",
       " 'Arschgepoppter',\n",
       " 'Arschger√ºst',\n",
       " 'Arschgesich',\n",
       " 'Arschgesicht',\n",
       " 'Arschgesichtpimmler',\n",
       " 'Arschgesigtesau',\n",
       " 'Arschgeweih',\n",
       " 'Arschgeweihbl√∂dmann',\n",
       " 'Arschgeweihverzierer',\n",
       " 'Arschgewichse',\n",
       " 'Arschgitarre',\n",
       " 'Arschgitarrenpfurzer',\n",
       " 'Arschgnom',\n",
       " 'Arschgrampe',\n",
       " 'Arschgranate',\n",
       " 'Arschgranatenlecker',\n",
       " 'Arschgrotte',\n",
       " 'Arschgrube',\n",
       " 'Arschgurke',\n",
       " 'Arschgurt',\n",
       " 'Arschg√ºrkchen',\n",
       " 'Arschhaar',\n",
       " 'Arschhaarbesamer',\n",
       " 'Arschhaarentferner',\n",
       " 'Arschhaarfetischist',\n",
       " 'Arschhaarflechter',\n",
       " 'Arschhaarfriseur',\n",
       " 'Arschhaarf√§rber',\n",
       " 'Arschhaarf√∂hner',\n",
       " 'Arschhaargl√§tter',\n",
       " 'Arschhaargnom',\n",
       " 'Arschhaarkauer',\n",
       " 'Arschhaarkiffer',\n",
       " 'Arschhaarkonsument',\n",
       " 'Arschhaark√§mmer',\n",
       " 'Arschhaarlockenz√§hler',\n",
       " 'Arschhaarm√§her',\n",
       " 'Arschhaarm√ºtze',\n",
       " 'Arschhaarrasierer',\n",
       " 'Arschhaarraucher',\n",
       " 'Arschhaarsortierer',\n",
       " 'Arschhaarstylelist',\n",
       " 'Arschhaarverehrer',\n",
       " 'Arschhaarwichser',\n",
       " 'Arschhaarzieher',\n",
       " 'Arschhaarzupfer',\n",
       " 'Arschhaarz√§hler',\n",
       " 'Arschhaken',\n",
       " 'Arschharrraucher',\n",
       " 'Arschhochhalter',\n",
       " 'Arschhoden',\n",
       " 'Arschhornhautabpuler',\n",
       " 'Arschhure',\n",
       " 'Arschhyperhero',\n",
       " 'Arschi',\n",
       " 'Arschibald',\n",
       " 'Arschificki',\n",
       " 'Arschiloge',\n",
       " 'Arschinhalierer',\n",
       " 'Arschiologe',\n",
       " 'Arschkacka',\n",
       " 'Arschkacke',\n",
       " 'Arschkackewurst',\n",
       " 'Arschkanickel',\n",
       " 'Arschkaninchen',\n",
       " 'Arschkanister',\n",
       " 'Arschkanone',\n",
       " 'Arschkante',\n",
       " 'Arschkantenlecker',\n",
       " 'Arschkan√ºle',\n",
       " 'Arschkarampengesicht',\n",
       " 'Arschkarotte',\n",
       " 'Arschkartoffel',\n",
       " 'Arschkastennascher',\n",
       " 'Arschkeibl',\n",
       " 'Arschkeks',\n",
       " 'Arschkeksler',\n",
       " 'Arschkerbe',\n",
       " 'Arschkind',\n",
       " 'Arschkipf',\n",
       " 'Arschkirsche',\n",
       " 'Arschklavier',\n",
       " 'Arschklotz',\n",
       " 'Arschknaller',\n",
       " 'Arschknecht',\n",
       " 'Arschknilch',\n",
       " 'Arschknispel',\n",
       " 'Arschknolle',\n",
       " 'Arschknoten',\n",
       " 'Arschkn√∂del',\n",
       " 'Arschkoffer',\n",
       " 'Arschkofpablutscher',\n",
       " 'Arschkopf',\n",
       " 'Arschkopfgeier',\n",
       " 'Arschkotzer',\n",
       " 'Arschkrammpe',\n",
       " 'Arschkrampe',\n",
       " 'Arschkrampendr√ºse',\n",
       " 'Arschkrampf',\n",
       " 'Arschkranate',\n",
       " 'Arschkrapfen',\n",
       " 'Arschkratzen',\n",
       " 'Arschkratzer',\n",
       " 'Arschkratzfetischist',\n",
       " 'Arschkrause',\n",
       " 'Arschkrawatte',\n",
       " 'Arschkrebs',\n",
       " 'Arschkricher',\n",
       " 'Arschkriecher',\n",
       " 'Arschkrokette',\n",
       " 'Arschkruste',\n",
       " 'Arschkrustenlecker',\n",
       " 'Arschkr√§tzenfresse',\n",
       " 'Arschkr√∂te',\n",
       " 'Arschkr√ºmel',\n",
       " 'Arschkr√ºmelficker',\n",
       " 'Arschkr√ºppel',\n",
       " 'Arschkuchen',\n",
       " 'Arschkuh',\n",
       " 'Arschlampe',\n",
       " 'Arschlappen',\n",
       " 'Arschleberfleck',\n",
       " 'Arschlecker',\n",
       " 'Arschleckfresse',\n",
       " 'Arschleckschwein',\n",
       " 'Arschling',\n",
       " 'Arschloch',\n",
       " 'Arschlochbeschw√∂rer',\n",
       " 'Arschlochentl√ºfter',\n",
       " 'Arschlochentz√ºndeter',\n",
       " 'Arschlocher',\n",
       " 'Arschlocheule',\n",
       " 'Arschlochfahrer',\n",
       " 'Arschlochficker',\n",
       " 'Arschlochfickfotze',\n",
       " 'Arschlochfurz',\n",
       " 'Arschlochfuttsau',\n",
       " 'Arschlochgesicht',\n",
       " 'Arschlochkatze',\n",
       " 'Arschlochkind',\n",
       " 'Arschlochkriecher',\n",
       " 'Arschlochl√∂cher',\n",
       " 'Arschlochmitl√§ufer',\n",
       " 'Arschlochmonster',\n",
       " 'Arschlochnachrenner',\n",
       " 'Arschlochpenner',\n",
       " 'Arschlochpolierer',\n",
       " 'Arschlochpuhler',\n",
       " 'Arschlochsau',\n",
       " 'Arschlochschei√üe',\n",
       " 'Arschlochsohn',\n",
       " 'Arschlochspalte',\n",
       " 'Arschlochvogel',\n",
       " 'Arschlochwarmhalter',\n",
       " 'Arschlochwichser',\n",
       " 'Arschlurch',\n",
       " 'Arschlutscher',\n",
       " 'Arschmade',\n",
       " 'Arschmakrele',\n",
       " 'Arschmeister',\n",
       " 'Arschmopser',\n",
       " 'Arschmuschi',\n",
       " 'Arschm√∂se',\n",
       " 'Arschm√∂senabwischer',\n",
       " 'Arschm√∂senschaum',\n",
       " 'Arschnadel',\n",
       " 'Arschnagler',\n",
       " 'Arschnase',\n",
       " 'Arschnutte',\n",
       " 'Arschologe',\n",
       " 'Arschotter',\n",
       " 'Arschparade',\n",
       " 'Arschpauker',\n",
       " 'Arschpenis',\n",
       " 'Arschpeniskopf',\n",
       " 'Arschpenislutscher',\n",
       " 'Arschpenner',\n",
       " 'Arschpfeife',\n",
       " 'Arschpickel',\n",
       " 'Arschpickelzersitzer',\n",
       " 'Arschpilz',\n",
       " 'Arschpilzsammler',\n",
       " 'Arschpimmel',\n",
       " 'Arschpimmelauge',\n",
       " 'Arschpimml',\n",
       " 'Arschpirat',\n",
       " 'Arschpisser',\n",
       " 'Arschpomp',\n",
       " 'Arschpopel',\n",
       " 'Arschpopeler',\n",
       " 'Arschpopo',\n",
       " 'Arschpopper',\n",
       " 'Arschpoppler',\n",
       " 'Arschprinz',\n",
       " 'Arschprinzessin',\n",
       " 'Arschprolet',\n",
       " 'Arschpropeller',\n",
       " 'Arschpuler',\n",
       " 'Arschpunk',\n",
       " 'Arschpuppenkalamari',\n",
       " 'Arschpupser',\n",
       " 'Arschrakete',\n",
       " 'Arschrambe',\n",
       " 'Arschrammler',\n",
       " 'Arschrasierer',\n",
       " 'Arschratte',\n",
       " 'Arschregelbluter',\n",
       " 'Arschriecher',\n",
       " 'Arschringlutscher',\n",
       " 'Arschritzenfummler',\n",
       " 'Arschritzenkehrmaschine',\n",
       " 'Arschritzenkrauler',\n",
       " 'Arschritzenlecker',\n",
       " 'Arschritzenrasierer',\n",
       " 'Arschritzenricher',\n",
       " 'Arschritzensackkopf',\n",
       " 'Arschritzerputzer',\n",
       " 'Arschrollade',\n",
       " 'Arschrosette',\n",
       " 'Arschsack',\n",
       " 'Arschsaftlutscher',\n",
       " 'Arschsau',\n",
       " 'Arschsauger',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
